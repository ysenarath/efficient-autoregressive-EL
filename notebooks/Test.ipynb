{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c056d36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%` not found.\n"
     ]
    }
   ],
   "source": [
    "% load_ext autoreload\n",
    "% autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4fc6a72",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "759b3485",
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import ArgumentParser\n",
    "from pytorch_lightning import Trainer\n",
    "from entlink.model.efficient_el import EfficientEL\n",
    "from entlink.utils import get_markdown\n",
    "\n",
    "from IPython.display import Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7a51eca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yasas/Documents/Projects/ysenarath/efficient-autoregressive-EL/venv/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:658: UserWarning: You passed `Trainer(accelerator='cpu', precision=16)` but native AMP is not supported on CPU. Using `precision='bf16'` instead.\n",
      "  rank_zero_warn(\n",
      "Using bfloat16 Automatic Mixed Precision (AMP)\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    }
   ],
   "source": [
    "parser = ArgumentParser()\n",
    "\n",
    "parser = Trainer.add_argparse_args(parser)\n",
    "\n",
    "args, _ = parser.parse_known_args()\n",
    "args.gpus = 0\n",
    "args.precision = 16\n",
    "\n",
    "trainer = Trainer.from_argparse_args(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2e625161",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerForMaskedLM: ['longformer.encoder.layer.9.attention.self.value_global.weight', 'longformer.encoder.layer.8.attention.self.query_global.bias', 'longformer.encoder.layer.10.attention.self.key.bias', 'longformer.encoder.layer.10.attention.self.value.weight', 'longformer.encoder.layer.9.attention.self.key.weight', 'longformer.encoder.layer.11.attention.output.dense.bias', 'longformer.encoder.layer.11.attention.self.value.bias', 'longformer.encoder.layer.8.output.dense.bias', 'longformer.encoder.layer.8.output.dense.weight', 'longformer.encoder.layer.11.attention.self.value_global.bias', 'longformer.encoder.layer.8.intermediate.dense.bias', 'longformer.encoder.layer.8.attention.output.dense.bias', 'longformer.encoder.layer.11.attention.self.query.weight', 'longformer.encoder.layer.11.attention.self.key_global.bias', 'longformer.encoder.layer.9.attention.self.query_global.weight', 'longformer.encoder.layer.9.attention.self.query.bias', 'longformer.encoder.layer.9.attention.self.value.bias', 'longformer.encoder.layer.9.attention.output.dense.bias', 'longformer.encoder.layer.9.attention.self.query_global.bias', 'longformer.encoder.layer.10.attention.output.dense.bias', 'longformer.encoder.layer.8.attention.self.query.bias', 'longformer.encoder.layer.8.attention.self.value.weight', 'longformer.encoder.layer.11.attention.output.dense.weight', 'longformer.encoder.layer.11.intermediate.dense.weight', 'longformer.encoder.layer.8.attention.self.key_global.weight', 'longformer.encoder.layer.9.attention.self.key_global.weight', 'longformer.encoder.layer.8.attention.output.dense.weight', 'longformer.encoder.layer.10.output.LayerNorm.weight', 'longformer.encoder.layer.8.attention.self.key.weight', 'longformer.encoder.layer.8.attention.self.value.bias', 'longformer.encoder.layer.10.attention.self.value.bias', 'longformer.encoder.layer.9.attention.self.key.bias', 'longformer.encoder.layer.10.intermediate.dense.bias', 'longformer.encoder.layer.10.attention.self.query.weight', 'longformer.encoder.layer.11.attention.self.value_global.weight', 'longformer.encoder.layer.8.attention.self.query.weight', 'longformer.encoder.layer.10.output.dense.bias', 'longformer.encoder.layer.9.attention.output.LayerNorm.bias', 'longformer.encoder.layer.8.attention.output.LayerNorm.weight', 'longformer.encoder.layer.8.intermediate.dense.weight', 'longformer.encoder.layer.11.attention.self.query_global.bias', 'longformer.encoder.layer.9.attention.self.key_global.bias', 'longformer.encoder.layer.10.attention.self.key.weight', 'longformer.encoder.layer.10.output.LayerNorm.bias', 'longformer.encoder.layer.9.intermediate.dense.weight', 'longformer.encoder.layer.10.attention.self.query_global.weight', 'longformer.encoder.layer.9.output.dense.weight', 'longformer.encoder.layer.9.attention.self.query.weight', 'longformer.encoder.layer.10.attention.self.value_global.bias', 'longformer.encoder.layer.10.attention.output.LayerNorm.weight', 'longformer.encoder.layer.9.attention.output.dense.weight', 'longformer.encoder.layer.10.attention.self.query.bias', 'longformer.encoder.layer.11.attention.self.value.weight', 'longformer.encoder.layer.8.attention.self.query_global.weight', 'longformer.encoder.layer.11.output.LayerNorm.weight', 'longformer.encoder.layer.10.output.dense.weight', 'longformer.encoder.layer.11.output.LayerNorm.bias', 'longformer.encoder.layer.8.output.LayerNorm.weight', 'longformer.encoder.layer.9.attention.self.value_global.bias', 'longformer.encoder.layer.10.attention.self.query_global.bias', 'longformer.encoder.layer.8.attention.self.key.bias', 'longformer.encoder.layer.8.output.LayerNorm.bias', 'longformer.encoder.layer.10.attention.self.key_global.weight', 'longformer.encoder.layer.11.attention.self.query_global.weight', 'longformer.encoder.layer.11.output.dense.weight', 'longformer.encoder.layer.10.attention.output.dense.weight', 'longformer.encoder.layer.11.attention.self.query.bias', 'longformer.encoder.layer.9.intermediate.dense.bias', 'longformer.encoder.layer.9.output.dense.bias', 'longformer.encoder.layer.11.attention.self.key_global.weight', 'longformer.encoder.layer.10.intermediate.dense.weight', 'longformer.encoder.layer.8.attention.self.key_global.bias', 'longformer.encoder.layer.11.output.dense.bias', 'longformer.encoder.layer.8.attention.self.value_global.bias', 'longformer.encoder.layer.10.attention.self.key_global.bias', 'longformer.encoder.layer.10.attention.output.LayerNorm.bias', 'longformer.encoder.layer.9.output.LayerNorm.bias', 'longformer.encoder.layer.9.attention.output.LayerNorm.weight', 'longformer.encoder.layer.9.output.LayerNorm.weight', 'longformer.encoder.layer.11.attention.self.key.weight', 'longformer.encoder.layer.11.intermediate.dense.bias', 'longformer.encoder.layer.11.attention.output.LayerNorm.weight', 'longformer.encoder.layer.10.attention.self.value_global.weight', 'longformer.encoder.layer.11.attention.output.LayerNorm.bias', 'longformer.encoder.layer.9.attention.self.value.weight', 'longformer.encoder.layer.11.attention.self.key.bias', 'longformer.encoder.layer.8.attention.self.value_global.weight', 'longformer.encoder.layer.8.attention.output.LayerNorm.bias']\n",
      "- This IS expected if you are initializing LongformerForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LongformerForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of LongformerForMaskedLM were not initialized from the model checkpoint at allenai/longformer-base-4096 and are newly initialized: ['lm_head.decoder.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "kwargs = {\n",
    "    'test_data_path': '../data/aida_test_dataset.jsonl',\n",
    "    'mentions_filename': '../data/mentions.json',\n",
    "    'entities_filename': '../data/entities.json',\n",
    "}\n",
    "\n",
    "model = EfficientEL.load_from_checkpoint('../models/model.ckpt', **kwargs).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ea367df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.hparams.threshold = -3.2\n",
    "model.hparams.test_with_beam_search = False\n",
    "model.hparams.test_with_beam_search_no_candidates = False\n",
    "# trainer.test(model, test_dataloaders=model.test_dataloader(), ckpt_path=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ec5eeeb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e215ecb86bd643d692245e6fe894fd21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading ..:   0%|          | 0/470105 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.generate_global_trie()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e259e626",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0,\n",
       "   17,\n",
       "   [('United States', -2.541071891784668),\n",
       "    ('United Airlines', -4.048898220062256),\n",
       "    ('United Kingdom', -4.068247318267822),\n",
       "    ('American Airlines', -6.304810047149658),\n",
       "    ('United States Department of Agriculture', -12.176301002502441)])]]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = \"American Airlines is a fine airline.\"\n",
    "\n",
    "model.sample([s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "908a75ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "[American Airlines](https://en.wikipedia.org/wiki/United_States) is a fine airline."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(get_markdown([s], [[(s[0], s[1], s[2][0][0]) for s in spans] for spans in model.sample([s])])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2bc2d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
